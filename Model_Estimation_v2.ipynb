{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_stata('/Users/nbs/Documents/Georgetown/Semester 5/1 Courses/GBUS 401/1 Project/gbus_401_project/Data_Final/gbus_401_project_master.dta')\n",
    "\n",
    "# Extract variables of interest\n",
    "df = df[['year', 'school_id', 'admit', 'gpa', 'lsat', 'urm', 'fee_waived', 'non_trad', 'intl']]\n",
    "\n",
    "# Drop observations with missing variables\n",
    "df = df.dropna(axis='index') # Drop missing\n",
    "\n",
    "# Convert year, school_id to dummies for TWFE\n",
    "df_year = df[['year']]\n",
    "df = pd.get_dummies(df, prefix=['y', 'sid'], columns=['year', 'school_id'], drop_first=True) # First column is dropped to prevent collinearity\n",
    "df = df.join(df_year)\n",
    "\n",
    "# Clean up\n",
    "df = df.replace(['False', 'True'], [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics, cross_validation, accuracy_score\n",
    "\n",
    "\n",
    "predicted = cross_validation.cross_val_predict(LogisticRegression(), X, y, cv=10)\n",
    "print metrics.accuracy_score(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and outcome\n",
    "y = df[['admit']]\n",
    "X = df.drop(['admit'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m ccp_alpha \u001b[39min\u001b[39;00m ccp_alphas:\n\u001b[1;32m     12\u001b[0m     clf \u001b[39m=\u001b[39m DecisionTreeClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, ccp_alpha\u001b[39m=\u001b[39mccp_alpha)\n\u001b[0;32m---> 13\u001b[0m     clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     14\u001b[0m     clfs\u001b[39m.\u001b[39mappend(clf)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Remove trivial tree with one node\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/sklearn/tree/_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    940\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \n\u001b[1;32m    942\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    970\u001b[0m         X,\n\u001b[1;32m    971\u001b[0m         y,\n\u001b[1;32m    972\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    973\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    974\u001b[0m     )\n\u001b[1;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/sklearn/tree/_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    449\u001b[0m         splitter,\n\u001b[1;32m    450\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    456\u001b[0m     )\n\u001b[0;32m--> 458\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Split data into testing (25%) and training (75%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Run decision tree\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "# Train decision tree using effective alphas\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "\n",
    "# Remove trivial tree with one node\n",
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "# Plot maximum depth vs. alpha\n",
    "max_depths = [clf.tree_.max_depth for clf in clfs]\n",
    "\n",
    "fig1 = plt.figure(dpi=150)\n",
    "plt.scatter(ccp_alphas, max_depths)\n",
    "plt.plot(ccp_alphas,max_depths, drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Maximum Depth\")\n",
    "plt.title(\"Tree Depth Decreases as Alpha Increases\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy vs. alpha\n",
    "train_scores = [clf.score(X_train, y_train) for clf in clfs] # What is the score?\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.set_xlabel(\"Alpha\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy and Alpha for Training and Testing Data\")\n",
    "ax.plot(ccp_alphas,train_scores,marker=\"o\",label=\"Train\",drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas,test_scores,marker=\"o\",label=\"Test\",drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and outcome\n",
    "y = df[['admit']]\n",
    "X = df.drop(['admit'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "gpa : 0.372\n",
      "lsat : 0.038\n",
      "urm : 0.151\n",
      "fee_waived : 0.047\n",
      "non_trad : -0.012\n",
      "intl : -0.055\n",
      "\n",
      "Intercept: -531490359818.616 \n",
      "\n",
      "Goodness of Fit\n",
      "Cross Entropy: 0.436\n",
      "R^2 0.445\n",
      "MSE: 0.12\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = LinearRegression(n_jobs=-1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict admit\n",
    "y_hat = np.array([i for i in model.predict(X)])\n",
    "\n",
    "# Print outputs\n",
    "print('Coefficients')\n",
    "[print(a, ':', round(b, 3)) for a, b in zip(model.feature_names_in_[0:6], model.coef_.flatten()[0:6])]\n",
    "print('')\n",
    "\n",
    "print('Intercept:', round(model.intercept_.item(), 3), '\\n')\n",
    "\n",
    "print('Goodness of Fit')\n",
    "print('Cross Entropy:', round(log_loss(y, y_hat), 3)) # issue caused by FE for some reason???\n",
    "print('R^2', round(model.score(X, y), 3))\n",
    "print('MSE:', round(mean_squared_error(y, y_hat), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to time series\n",
    "pd.to_datetime(df.year).dt.year\n",
    "df.set_index('year', inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Define X and y\n",
    "X = df.drop(labels=['admit'], axis=1)\n",
    "y = df[['admit']]\n",
    "\n",
    "tss = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     2 ... 62195 62196 62197] TEST: [ 62198  62199  62200 ... 124388 124389 124390]\n",
      "Accuracy for the fold no. 1 on the test set: -1.1213709733116754e+21\n",
      "TRAIN: [     0      1      2 ... 124388 124389 124390] TEST: [124391 124392 124393 ... 186581 186582 186583]\n",
      "Accuracy for the fold no. 1 on the test set: -2.9152024465258257e+20\n",
      "TRAIN: [     0      1      2 ... 186581 186582 186583] TEST: [186584 186585 186586 ... 248774 248775 248776]\n",
      "Accuracy for the fold no. 1 on the test set: -1.7296321942309955e+20\n",
      "TRAIN: [     0      1      2 ... 248774 248775 248776] TEST: [248777 248778 248779 ... 310967 310968 310969]\n",
      "Accuracy for the fold no. 1 on the test set: -3.964797203474756e+19\n",
      "TRAIN: [     0      1      2 ... 310967 310968 310969] TEST: [310970 310971 310972 ... 373160 373161 373162]\n",
      "Accuracy for the fold no. 1 on the test set: 0.4358899400976498\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(n_jobs=-1)\n",
    "model.fit(X, y)\n",
    "\n",
    "i = 1\n",
    "for train_index, test_index in tss.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index,:]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"Accuracy for the fold no. {i} on the test set: {model.score(X, y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fit_time', 'score_time', 'test_neg_mean_squared_error', 'test_r2']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X = df[['gpa', 'lsat', 'urm', 'fee_waived', 'non_trad', 'intl']]\n",
    "y = df[['admit']]\n",
    "\n",
    "model = LinearRegression(n_jobs=-1)\n",
    "# tss = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
    "\n",
    "scoring=('r2', 'neg_mean_squared_error')\n",
    "\n",
    "cv_results = cross_validate(model, X, y, cv=5, scoring=scoring)\n",
    "sorted(cv_results.keys())\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03932869,  0.06656639, -0.04264215,  0.02885161, -0.05557259])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test_r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.73767919253277e+20"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "lr = LinearRegression()\n",
    "np.mean(cross_val_score(lr, X, y, cv=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('gbus401project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16a8c26647576d847487f6b4d11474c9069db67d43b3c25a4a14d3925bdc6ef9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
