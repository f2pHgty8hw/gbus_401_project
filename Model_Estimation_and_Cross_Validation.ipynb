{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df0 = pd.read_stata('/Users/nbs/Documents/Georgetown/Semester 5/1 Courses/GBUS 401/1 Project/gbus_401_project/Data_Final/gbus_401_project_master.dta')\n",
    "\n",
    "# Clean up\n",
    "df = df0.replace(['False', 'True'], [0, 1])\n",
    "\n",
    "# Convert year, school_id to dummies for TWFE\n",
    "fe_catvars = df[['year', 'school_id']]\n",
    "df = pd.get_dummies(df, prefix=['y', 'sid'], columns=['year', 'school_id'], drop_first=True) # First column is dropped to prevent collinearity\n",
    "df = df.join(fe_catvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to use\n",
    "varlist = ['admit', 'gpa', 'lsat', 'urm', 'fee_waived', 'non_trad', 'intl', 'year']\n",
    "\n",
    "# Fixed-effect dummies\n",
    "for i in list(df.columns):\n",
    "    if ('sid_' in i) or ('y_' in i):\n",
    "        varlist.append(i)\n",
    "\n",
    "# Define dataset for Model 1\n",
    "df1 = df[varlist]\n",
    "df1 = df1.dropna(axis='index') # Drop missing\n",
    "\n",
    "# Define features and outcome\n",
    "y = df1[['admit']]\n",
    "X = df1.drop(['admit', 'year'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "gpa : 0.372\n",
      "lsat : 0.038\n",
      "urm : 0.151\n",
      "fee_waived : 0.047\n",
      "non_trad : -0.012\n",
      "intl : -0.055\n",
      "\n",
      "Intercept: -6.04 \n",
      "\n",
      "Goodness of Fit\n",
      "Cross Entropy: 0.436\n",
      "R^2 0.445\n",
      "MSE: 0.12\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = LinearRegression(n_jobs=-1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict admit\n",
    "y_hat = np.array([i for i in model.predict(X)])\n",
    "\n",
    "# Print outputs\n",
    "print('Coefficients')\n",
    "[print(a, ':', round(b, 3)) for a, b in zip(model.feature_names_in_[0:6], model.coef_.flatten()[0:6])]\n",
    "print('')\n",
    "\n",
    "print('Intercept:', round(model.intercept_.item(), 3), '\\n')\n",
    "\n",
    "print('Goodness of Fit')\n",
    "print('Cross Entropy:', round(log_loss(y, y_hat), 3)) # issue caused by FE for some reason???\n",
    "print('R^2', round(model.score(X, y), 3))\n",
    "print('MSE:', round(mean_squared_error(y, y_hat), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# All possible features\n",
    "features = ['gpa', 'lsat', 'urm', 'fee_waived', 'non_trad', 'intl']\n",
    "\n",
    "# Get all combinations of features to test\n",
    "feature_combos = []\n",
    "for j in range(1, len(features) + 1):\n",
    "    feature_combos.append(list(itertools.combinations(features, j)))\n",
    "feature_combos = list(itertools.chain.from_iterable(feature_combos))\n",
    "print(type(list(feature_combos[0])))\n",
    "\n",
    "# Get year and FEs\n",
    "fes = df1.columns[7:].values.tolist()\n",
    "print(type(fes))\n",
    "\n",
    "# Get year\n",
    "y = ['admit']\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gpa'], 'year', 'y_2005', 'y_2006', 'y_2007', 'y_2008', 'y_2009', 'y_2010', 'y_2011', 'y_2012', 'y_2013', 'y_2014', 'y_2015', 'y_2016', 'y_2017', 'y_2018', 'y_2019', 'y_2020', 'y_2021', 'y_2022', 'y_2023', 'sid_103600', 'sid_105100', 'sid_108100', 'sid_108300', 'sid_110101', 'sid_110800', 'sid_116453', 'sid_120529', 'sid_121601', 'sid_123456', 'sid_129500', 'sid_130503', 'sid_131273', 'sid_131373', 'sid_131400', 'sid_131500', 'sid_132500', 'sid_132600', 'sid_132800', 'sid_132901', 'sid_134201', 'sid_137000', 'sid_137100', 'sid_140201', 'sid_141706', 'sid_142600', 'sid_143401', 'sid_143711', 'sid_144102', 'sid_144402', 'sid_144503', 'sid_144802', 'sid_146601', 'sid_146800', 'sid_148001', 'sid_148900', 'sid_150900', 'sid_153101', 'sid_153505', 'sid_153601', 'sid_156400', 'sid_157400', 'sid_158015', 'sid_159800', 'sid_161000', 'sid_162600', 'sid_167100', 'sid_169101', 'sid_169800', 'sid_171000', 'sid_173700', 'sid_173900', 'sid_175800', 'sid_177400', 'sid_177500', 'sid_180900', 'sid_181300', 'sid_184000', 'sid_184200', 'sid_186000', 'sid_189200', 'sid_194800', 'sid_194900', 'sid_198903', 'sid_199900', 'sid_201000', 'sid_201600', 'sid_202500', 'sid_202900', 'sid_205400', 'sid_210200', 'sid_210400', 'sid_212801', 'sid_213007', 'sid_215510', 'sid_219900', 'sid_221007', 'sid_221810', 'sid_222600', 'sid_225400', 'sid_232300', 'sid_232503', 'sid_232902', 'sid_234500', 'sid_239101', 'sid_241501', 'sid_244000', 'sid_250609', 'sid_251600', 'sid_251800', 'sid_252000', 'sid_253600', 'sid_254200', 'sid_256500', 'sid_256900', 'sid_258903', 'sid_262900', 'sid_263201', 'sid_266300', 'sid_267700', 'sid_270711', 'sid_271101', 'sid_272203', 'sid_273200', 'sid_278300', 'sid_278500', 'sid_279102', 'sid_282300', 'sid_283700', 'sid_288200', 'sid_288600', 'sid_290301', 'sid_291300', 'sid_292002', 'sid_292701', 'sid_295000', 'sid_297400', 'sid_297805', 'sid_300500', 'sid_302302', 'sid_302400', 'sid_303200', 'sid_308900', 'sid_309000', 'sid_312300', 'sid_312500', 'sid_312700', 'sid_313100', 'sid_316614', 'sid_318400', 'sid_318500', 'sid_319700', 'sid_322300', 'sid_322700', 'sid_325600', 'sid_325800', 'sid_331301', 'sid_331302', 'sid_332922', 'sid_337101', 'sid_337800', 'sid_337900', 'sid_338801', 'sid_341001', 'sid_344800', 'sid_347400', 'sid_347900', 'sid_350900', 'sid_353000', 'sid_353500', 'sid_354500', 'sid_361300', 'sid_362300', 'sid_364200', 'sid_364400', 'sid_364500', 'sid_365200', 'sid_365800', 'sid_367000', 'sid_367500', 'sid_370500', 'sid_374400', 'sid_374500', 'sid_374900', 'sid_376800', 'sid_377800', 'sid_379001', 'sid_379800', 'sid_382700', 'sid_386300', 'sid_389500', 'sid_393200', 'sid_393600', 'sid_393801', 'sid_394200', 'sid_394700', 'sid_396900', 'sid_497700', 'sid_891600', 'sid_927500', 'sid_963500', 'sid_1014205', 'sid_1014913', 'sid_1039501', 'sid_1085400', 'sid_1164901', 'sid_1193400', 'sid_1262700', 'sid_1310300', 'sid_2053000', 'sid_2179937', 'sid_3091300', 'sid_3173300', 'sid_3191300', 'sid_3374300', 'sid_3559300', 'sid_3691400', 'sid_4096300', 'sid_4131400', 'sid_4143500', 'sid_4242101', 'admit']\n",
      "Train: [2004] Test: [2005]\n",
      "Train: [2004, 2005] Test: [2006]\n",
      "Train: [2004, 2005, 2006] Test: [2007]\n",
      "Train: [2004, 2005, 2006, 2007] Test: [2008]\n",
      "Train: [2004, 2005, 2006, 2007, 2008] Test: [2009]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009] Test: [2010]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010] Test: [2011]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011] Test: [2012]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012] Test: [2013]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013] Test: [2014]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014] Test: [2015]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015] Test: [2016]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016] Test: [2017]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017] Test: [2018]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018] Test: [2019]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019] Test: [2020]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020] Test: [2021]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021] Test: [2022]\n",
      "Train: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022] Test: [2023]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nbs/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/pandas/core/common.py:245: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = np.asarray(values, dtype=dtype)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m entropies \u001b[39m=\u001b[39m []\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(year_list) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m     X_train, X_test \u001b[39m=\u001b[39m splits[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m][k][flist], splits[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m][k][flist]\n\u001b[1;32m     51\u001b[0m     y_train, y_test \u001b[39m=\u001b[39m splits[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][k][[\u001b[39m'\u001b[39m\u001b[39madmit\u001b[39m\u001b[39m'\u001b[39m]], splits[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m][k][[\u001b[39m'\u001b[39m\u001b[39madmit\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     53\u001b[0m     model\u001b[39m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/pandas/core/frame.py:3811\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3810\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3811\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3813\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/pandas/core/indexes/base.py:6108\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6105\u001b[0m     keyarr \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39masarray_tuplesafe(keyarr)\n\u001b[1;32m   6107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6108\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_indexer_for(keyarr)\n\u001b[1;32m   6109\u001b[0m     keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreindex(keyarr)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   6110\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/pandas/core/indexes/base.py:6095\u001b[0m, in \u001b[0;36mIndex.get_indexer_for\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6077\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   6078\u001b[0m \u001b[39mGuaranteed return of an indexer even when non-unique.\u001b[39;00m\n\u001b[1;32m   6079\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6092\u001b[0m \u001b[39marray([0, 2])\u001b[39;00m\n\u001b[1;32m   6093\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   6094\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6095\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_indexer(target)\n\u001b[1;32m   6096\u001b[0m indexer, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m   6097\u001b[0m \u001b[39mreturn\u001b[39;00m indexer\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/pandas/core/indexes/base.py:3974\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3969\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   3970\u001b[0m     \u001b[39mreturn\u001b[39;00m this\u001b[39m.\u001b[39m_get_indexer(\n\u001b[1;32m   3971\u001b[0m         target, method\u001b[39m=\u001b[39mmethod, limit\u001b[39m=\u001b[39mlimit, tolerance\u001b[39m=\u001b[39mtolerance\n\u001b[1;32m   3972\u001b[0m     )\n\u001b[0;32m-> 3974\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_indexer(target, method, limit, tolerance)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/pandas/core/indexes/base.py:4001\u001b[0m, in \u001b[0;36mIndex._get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3998\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3999\u001b[0m         tgt_values \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39m_get_engine_target()\n\u001b[0;32m-> 4001\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_indexer(tgt_values)\n\u001b[1;32m   4003\u001b[0m \u001b[39mreturn\u001b[39;00m ensure_platform_int(indexer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/pandas/_libs/index.pyx:308\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_indexer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5794\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.lookup\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Cross validation; cite: # https://stackoverflow.com/questions/58069691/how-to-create-a-train-test-split-of-time-series-data-by-year\n",
    "\n",
    "mr2s = []\n",
    "mentropies = []\n",
    "\n",
    "for i in feature_combos:\n",
    "\n",
    "    # Define data set\n",
    "    i = list(i)\n",
    "\n",
    "    flist = []\n",
    "\n",
    "    for j in i:\n",
    "        flist.append(i)\n",
    "\n",
    "    for j in fes:\n",
    "        flist.append(j)\n",
    "    \n",
    "    for j in y:\n",
    "        flist.append(j)\n",
    "\n",
    "    print(flist)\n",
    "    dfi = df1[features_i]\n",
    "\n",
    "    # Define X and y\n",
    "    y = dfi[['admit']]\n",
    "    X = dfi.drop(['admit', 'year'], axis=1)\n",
    "    \n",
    "    # Split data set\n",
    "    year_list = sorted(dfi['year'].unique())\n",
    "    splits = {'train': [], 'test': []}\n",
    "\n",
    "    for j, year in enumerate(year_list[:-1]):\n",
    "\n",
    "        train_year = year_list[:j + 1]\n",
    "        test_year = [year_list[j + 1]]\n",
    "\n",
    "        #print('Train:', train_year, 'Test:',test_year)\n",
    "        \n",
    "        splits['train'].append(dfi.loc[dfi.year.isin(train_year), :])\n",
    "        splits['test'].append(dfi.loc[dfi.year.isin(test_year), :])\n",
    "\n",
    "    # Estimate test statistics\n",
    "\n",
    "    r2s = []\n",
    "    entropies = []\n",
    "\n",
    "    for k in range(len(year_list) - 1):\n",
    "\n",
    "        X_train, X_test = splits['train'][k][flist], splits['test'][k][flist]\n",
    "        y_train, y_test = splits['train'][k][['admit']], splits['test'][k][['admit']]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        r2 = model.score(X_test, y_test)\n",
    "        r2s.append(r2)\n",
    "\n",
    "        y_test_hat = model.predict(X_test)\n",
    "        entropy = log_loss(y, y_test_hat) # WHY WON'T THIS WORK?????\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    mr2s.append(np.mean(r2s))\n",
    "    mentropies.append(np.mean(entropies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on test and train data sets\n",
    "\n",
    "\n",
    "\n",
    "# List of variables to use\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpa', 'lsat']\n"
     ]
    }
   ],
   "source": [
    "print(list(list(itertools.chain.from_iterable(features))[15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Decision Tree\n",
    "\n",
    "See here for source: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into testing (25%) and training (75%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Run decision tree\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "# Train decision tree using effective alphas\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "\n",
    "# Remove trivial tree with one node\n",
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "# Plot maximum depth vs. alpha\n",
    "max_depths = [clf.tree_.max_depth for clf in clfs]\n",
    "\n",
    "fig1 = plt.figure(dpi=150)\n",
    "plt.scatter(ccp_alphas, max_depths)\n",
    "plt.plot(ccp_alphas,max_depths, drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Maximum Depth\")\n",
    "plt.title(\"Tree Depth Decreases as Alpha Increases\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy vs. alpha\n",
    "train_scores = [clf.score(X_train, y_train) for clf in clfs] # What is the score?\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.set_xlabel(\"Alpha\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy and Alpha for Training and Testing Data\")\n",
    "ax.plot(ccp_alphas,train_scores,marker=\"o\",label=\"Train\",drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas,test_scores,marker=\"o\",label=\"Test\",drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('gbus401project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16a8c26647576d847487f6b4d11474c9069db67d43b3c25a4a14d3925bdc6ef9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
