{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df0 = pd.read_stata('/Users/nbs/Documents/Georgetown/Semester 5/1 Courses/GBUS 401/1 Project/gbus_401_project_master.dta')\n",
    "\n",
    "# Clean up\n",
    "df = df0.replace(['False', 'True'], [0, 1])\n",
    "\n",
    "# Convert year, school_id to dummies for TWFE\n",
    "fe_vars = df[['school_id', 'year']]\n",
    "df = pd.get_dummies(df, prefix=['y', 'sid'], columns=['year', 'school_id'], drop_first=True) # First column is dropped to prevent collinearity\n",
    "df = df.join(fe_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to use\n",
    "varlist = ['admit', 'gpa', 'lsat', 'urm', 'fee_waived', 'non_trad', 'intl', 'year']\n",
    "\n",
    "# Fixed-effect dummies\n",
    "for i in list(df.columns):\n",
    "    if ('sid_' in i) or ('y_' in i):\n",
    "        varlist.append(i)\n",
    "\n",
    "# Define dataset for Model 1\n",
    "df1 = df[varlist]\n",
    "df1 = df1.dropna(axis='index') # Drop missing\n",
    "\n",
    "# Define features and outcome\n",
    "y = df1[['admit']]\n",
    "X = df1.drop(['admit', 'year'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "gpa : 0.372\n",
      "lsat : 0.038\n",
      "urm : 0.151\n",
      "fee_waived : 0.047\n",
      "non_trad : -0.012\n",
      "intl : -0.055\n",
      "\n",
      "Intercept: -6.04 \n",
      "\n",
      "Goodness of Fit\n",
      "Cross Entropy: 0.436\n",
      "R^2 0.445\n",
      "MSE: 0.12\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = LinearRegression(n_jobs=-1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict admit\n",
    "y_hat = np.array([i for i in model.predict(X)])\n",
    "\n",
    "# Print outputs\n",
    "print('Coefficients')\n",
    "[print(a, ':', round(b, 3)) for a, b in zip(model.feature_names_in_[0:6], model.coef_.flatten()[0:6])]\n",
    "print('')\n",
    "\n",
    "print('Intercept:', round(model.intercept_.item(), 3), '\\n')\n",
    "\n",
    "print('Goodness of Fit')\n",
    "print('Cross Entropy:', round(log_loss(y, y_hat), 3))\n",
    "print('R^2', round(model.score(X, y), 3))\n",
    "print('MSE:', round(mean_squared_error(y, y_hat), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation; cite: # https://stackoverflow.com/questions/58069691/how-to-create-a-train-test-split-of-time-series-data-by-year\n",
    "\n",
    "# Split data set\n",
    "year_list = sorted(df1['year'].unique())\n",
    "splits = {'train': [], 'test': []}\n",
    "\n",
    "for i, year in enumerate(year_list[:-1]):\n",
    "\n",
    "    train_year = year_list[:i + 1]\n",
    "    test_year = [year_list[i + 1]]\n",
    "\n",
    "    # print('Train:', train_year, 'Test:',test_year)\n",
    "    \n",
    "    splits['train'].append(df1.loc[df1.year.isin(train_year), :])\n",
    "    splits['test'].append(df1.loc[df1.year.isin(test_year), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 10556\n",
      "X_test: 18298\n",
      "[[1.00002058e+00]\n",
      " [1.00002058e+00]\n",
      " [1.00002058e+00]\n",
      " ...\n",
      " [2.05786571e-05]\n",
      " [2.05786571e-05]\n",
      " [2.05786571e-05]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [18298, 373163]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m y_test_hat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(y_test_hat)\n\u001b[0;32m---> 22\u001b[0m entropy \u001b[39m=\u001b[39m log_loss(y, y_test_hat)\n\u001b[1;32m     23\u001b[0m entropies\u001b[39m.\u001b[39mappend(entropy)\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(year_list[i])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2403\u001b[0m, in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Log loss, aka logistic loss or cross-entropy loss.\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m \n\u001b[1;32m   2336\u001b[0m \u001b[39mThis is the loss function used in (multinomial) logistic regression\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2400\u001b[0m \u001b[39m0.21616...\u001b[39;00m\n\u001b[1;32m   2401\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2402\u001b[0m y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 2403\u001b[0m check_consistent_length(y_pred, y_true, sample_weight)\n\u001b[1;32m   2405\u001b[0m lb \u001b[39m=\u001b[39m LabelBinarizer()\n\u001b[1;32m   2407\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/sklearn/utils/validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    390\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [18298, 373163]"
     ]
    }
   ],
   "source": [
    "# Run on test and train data sets\n",
    "\n",
    "model = LinearRegression(n_jobs=-1)\n",
    "\n",
    "r2s = []\n",
    "entropies = []\n",
    "\n",
    "for i in range(len(year_list) - 1):\n",
    "\n",
    "    X_train, X_test = splits['train'][i][varlist], splits['test'][i][varlist]\n",
    "    y_train, y_test = splits['train'][i][['admit']], splits['test'][i][['admit']]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    r2 = model.score(X_test, y_test)\n",
    "    r2s.append(r2)\n",
    "\n",
    "    print('X_train:',len(X_train))\n",
    "    print('X_test:',len(X_test))\n",
    "    y_test_hat = model.predict(X_test)\n",
    "    print(y_test_hat)\n",
    "    entropy = log_loss(y, y_test_hat)\n",
    "    entropies.append(entropy)\n",
    "\n",
    "    print(year_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.559\n",
      "Cross Entropy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nbs/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/nbs/opt/anaconda3/envs/gbus401project/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print('R^2:', round(np.mean(r2s), 3))\n",
    "print('Cross Entropy:', round(np.mean(entropies), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Decision Tree\n",
    "\n",
    "See here for source: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into testing (25%) and training (75%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Run decision tree\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "# Train decision tree using effective alphas\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "\n",
    "# Remove trivial tree with one node\n",
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "# Plot maximum depth vs. alpha\n",
    "max_depths = [clf.tree_.max_depth for clf in clfs]\n",
    "\n",
    "fig1 = plt.figure(dpi=150)\n",
    "plt.scatter(ccp_alphas, max_depths)\n",
    "plt.plot(ccp_alphas,max_depths, drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Maximum Depth\")\n",
    "plt.title(\"Tree Depth Decreases as Alpha Increases\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy vs. alpha\n",
    "train_scores = [clf.score(X_train, y_train) for clf in clfs] # What is the score?\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.set_xlabel(\"Alpha\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy and Alpha for Training and Testing Data\")\n",
    "ax.plot(ccp_alphas,train_scores,marker=\"o\",label=\"Train\",drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas,test_scores,marker=\"o\",label=\"Test\",drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('gbus401project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16a8c26647576d847487f6b4d11474c9069db67d43b3c25a4a14d3925bdc6ef9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
